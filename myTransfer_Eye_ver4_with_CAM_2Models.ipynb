{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from multiprocessing import Pool\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "from IPython.display import Image, display\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import random\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path_uid_to_cls = \"imagenet_2012_challenge_label_map_proto.pbtxt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_dir = '../Inception/Inception_model/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Inception:\n",
    "    tensor_name_input_jpeg = \"DecodeJpeg/contents:0\"\n",
    "    tensor_name_input_image = \"DecodeJpeg:0\"\n",
    "    tensor_name_resized_image = \"ResizeBilinear:0\"\n",
    "    tensor_name_softmax = \"softmax:0\"\n",
    "    tensor_name_softmax_logits = \"softmax/logits:0\"\n",
    "    tensor_name_transfer_layer = \"pool_3:0\"\n",
    "    tensor_name_transfer_conv_layer = 'mixed_10/join/concat_dim:0'\n",
    "    tensor_name_transfer_convJoin_layer = \"mixed_10/join:0\"\n",
    "    tensor_name_transfer_convMix_layer=\"mixed_10/tower_2/conv:0\"\n",
    "\n",
    "    def __init__(self ,data_dir):\n",
    "        self.name_lookup = NameLookup( data_dir)\n",
    "        self.graph = tf.Graph()\n",
    "        with self.graph.as_default():\n",
    "            path = os.path.join(data_dir, path_graph_def)\n",
    "            with tf.gfile.FastGFile(path, 'rb') as file:\n",
    "                graph_def = tf.GraphDef()\n",
    "                graph_def.ParseFromString(file.read())\n",
    "                tf.import_graph_def(graph_def, name='')\n",
    "        self.y_pred             = self.graph.get_tensor_by_name(self.tensor_name_softmax)\n",
    "        self.y_logits           = self.graph.get_tensor_by_name(self.tensor_name_softmax_logits)\n",
    "        self.resized_image      = self.graph.get_tensor_by_name(self.tensor_name_resized_image)\n",
    "        self.transfer_layer     = self.graph.get_tensor_by_name(self.tensor_name_transfer_layer)\n",
    "        self.transfer_conv_layer = self.graph.get_tensor_by_name(self.tensor_name_transfer_conv_layer)\n",
    "        self.transfer_convJoin_layer = self.graph.get_tensor_by_name(self.tensor_name_transfer_convJoin_layer)\n",
    "        self.transfer_convMix_layer=self.graph.get_tensor_by_name(\"mixed_10/tower_2/conv:0\")\n",
    "        self.transfer_len = self.transfer_layer.get_shape()[3]\n",
    "        self.transfer_conv_len = self.transfer_conv_layer.get_shape()\n",
    "        \n",
    "        self.session = tf.Session(graph=self.graph)\n",
    "        \n",
    "    def show_all_op(self):\n",
    "        all_ops=self.session.graph.get_operations()\n",
    "        for ele in all_ops:\n",
    "            print ele.name\n",
    "    def close(self):\n",
    "        self.session.close()\n",
    "    def _write_summary(self, logdir='summary/'):\n",
    "        writer = tf.train.SummaryWriter(logdir=logdir, graph=self.graph)\n",
    "        writer.close()\n",
    "    def _create_feed_dict(self, image_path=None, image=None):\n",
    "        if image is not None:\n",
    "            feed_dict = {self.tensor_name_input_image: image}\n",
    "        elif image_path is not None:\n",
    "            image_data = tf.gfile.FastGFile(image_path, 'rb').read()\n",
    "            feed_dict = {self.tensor_name_input_jpeg: image_data}\n",
    "        else:\n",
    "            raise ValueError(\"Either image or image_path must be set.\")\n",
    "        return feed_dict\n",
    "\n",
    "    def classify(self, image_path=None, image=None):\n",
    "        feed_dict = self._create_feed_dict(image_path=image_path, image=image)\n",
    "        pred = self.session.run(self.y_pred, feed_dict=feed_dict)\n",
    "        pred = np.squeeze(pred)\n",
    "        return pred\n",
    "\n",
    "    def get_resized_image(self, image_path=None, image=None):\n",
    "        feed_dict = self._create_feed_dict(image_path=image_path, image=image)\n",
    "        resized_image = self.session.run(self.resized_image, feed_dict=feed_dict)\n",
    "        resized_image = resized_image.squeeze(axis=0)\n",
    "        resized_image = resized_image.astype(float) / 255.0\n",
    "        return resized_image\n",
    "\n",
    "    def print_scores(self, pred, k=10, only_first_name=True):\n",
    "        print 'print_score'\n",
    "        idx = pred.argsort()\n",
    "        print idx\n",
    "        top_k = idx[-k:]\n",
    "        print top_k\n",
    "        for cls in reversed(top_k):\n",
    "            name = self.name_lookup.cls_to_name(cls=cls, only_first_name=only_first_name)\n",
    "            score = pred[cls]\n",
    "            print name , score\n",
    "            print(\"{0:>6.2%} : {1}\".format(score, name))\n",
    "    \n",
    "    def transfer_values(self, image_path=None, image=None):\n",
    "        feed_dict = self._create_feed_dict(image_path=image_path, image=image)\n",
    "        transfer_values = self.session.run(self.transfer_layer , feed_dict=feed_dict)\n",
    "        #transfer_values = np.squeeze(transfer_values)\n",
    "        return transfer_values\n",
    "    def transfer_conv_values(self , image_path = None , image = None):\n",
    "        feed_dict = self._create_feed_dict(image_path = image_path , image = image)\n",
    "        transfer_values = self.session.run(self.transfer_conv_layer , feed_dict=feed_dict)\n",
    "        #transfer_values = np.squeeze(transfer_values)\n",
    "        return transfer_values\n",
    "    def transfer_convJoin_values(self , image_path = None , image = None):\n",
    "        feed_dict = self._create_feed_dict(image_path = image_path , image = image)\n",
    "        transfer_values = self.session.run(self.transfer_convJoin_layer , feed_dict=feed_dict)\n",
    "        #transfer_values = np.squeeze(transfer_values)\n",
    "        return transfer_values\n",
    "    def transfer_convMix_values(self , image_path = None , image = None):\n",
    "        feed_dict = self._create_feed_dict(image_path = image_path , image = image)\n",
    "        transfer_values = self.session.run(self.transfer_convMix_layer , feed_dict=feed_dict)\n",
    "        #transfer_values = np.squeeze(transfer_values)\n",
    "        return transfer_values\n",
    "#    def transfer_mixed10_tower1_conv1(self , image_path = None , image = None)#mixed_10/tower_1/conv_1\n",
    "#        feed_dict = self._create_feed_dict(image_path = image_path , image = image)\n",
    "#        transfer_values = self.session.run(self.transfer_convMix_layer , feed_dict=feed_dict)\n",
    "\n",
    "def process_images(fn, images=None, image_paths=None):\n",
    "        using_images = images is not None\n",
    "        if using_images:\n",
    "            num_images = len(images)\n",
    "        else:\n",
    "            num_images = len(image_paths)\n",
    "        result = [None] * num_images\n",
    "        for i in range(num_images):\n",
    "            msg = \"\\r- Processing image: {0:>6} / {1}\".format(i+1, num_images)\n",
    "            sys.stdout.write(msg)\n",
    "            sys.stdout.flush()\n",
    "            if using_images:\n",
    "                result[i] = fn(image=images[i])\n",
    "            else:\n",
    "                result[i] = fn(image_path=image_paths[i])\n",
    "        print()\n",
    "        result = np.array(result)\n",
    "        return result\n",
    "    \n",
    "def transfer_values_cache(cache_path, model, images=None, image_paths=None):\n",
    "        def fn():\n",
    "            return process_images(fn=model.transfer_values, images=images, image_paths=image_paths)\n",
    "        transfer_values = cache(cache_path=cache_path, fn=fn)\n",
    "        return transfer_values\n",
    "def transfer_conv_values_cache(cache_path, model, images=None, image_paths=None):\n",
    "        def fn():\n",
    "            return process_images(fn=model.transfer_conv_values, images=images, image_paths=image_paths)\n",
    "        transfer_values = cache(cache_path=cache_path, fn=fn)\n",
    "        return transfer_values\n",
    "def transfer_convJoin_values_cache(cache_path, model, images=None, image_paths=None):\n",
    "    def fn():\n",
    "        return process_images(fn=model.transfer_convJoin_values, images=images, image_paths=image_paths)\n",
    "    transfer_values = cache(cache_path=cache_path, fn=fn)\n",
    "    return transfer_values\n",
    "def transfer_convMix_values_cache(cache_path, model, images=None, image_paths=None):\n",
    "    def fn():\n",
    "        return process_images(fn=model.transfer_convMix_values, images=images, image_paths=image_paths)\n",
    "    transfer_values = cache(cache_path=cache_path, fn=fn)\n",
    "    return transfer_values\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing expensive_function() ...\n",
      "- Data saved to cache-file: cache_expensive_function.pkl\n",
      "('result =', 56088)\n",
      "()\n",
      "Creating object from ExpensiveClass() ...\n",
      "- Data saved to cache-file: cache_ExpensiveClass.pkl\n",
      "('c =', 123)\n",
      "('d =', 456)\n",
      "('result = c * d =', 56088)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "def cache(cache_path, fn, *args, **kwargs):\n",
    "\n",
    "    if os.path.exists(cache_path):\n",
    "        # Load the cached data from the file.\n",
    "        with open(cache_path, mode='rb') as file:\n",
    "            obj = pickle.load(file)\n",
    "        print(\"- Data loaded from cache-file: \" + cache_path)\n",
    "    else:\n",
    "        obj = fn(*args, **kwargs)\n",
    "        with open(cache_path, mode='wb') as file:\n",
    "            pickle.dump(obj, file)\n",
    "        print(\"- Data saved to cache-file: \" + cache_path)\n",
    "    return obj\n",
    "\n",
    "def convert_numpy2pickle(in_path, out_path):\n",
    "    data = np.load(in_path)\n",
    "    with open(out_path, mode='wb') as file:\n",
    "        pickle.dump(data, file)\n",
    "        \n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    def expensive_function(a, b):\n",
    "        return a * b\n",
    "    print('Computing expensive_function() ...')\n",
    "    result = cache(cache_path='cache_expensive_function.pkl',\n",
    "                   fn=expensive_function, a=123, b=456)\n",
    "    print('result =', result)\n",
    "    print()\n",
    "    class ExpensiveClass:\n",
    "        def __init__(self, c, d):\n",
    "            self.c = c\n",
    "            self.d = d\n",
    "            self.result = c * d\n",
    "\n",
    "        def print_result(self):\n",
    "            print('c =', self.c)\n",
    "            print('d =', self.d)\n",
    "            print('result = c * d =', self.result)\n",
    "\n",
    "    print('Creating object from ExpensiveClass() ...')\n",
    "    obj = cache(cache_path='cache_ExpensiveClass.pkl',\n",
    "                fn=ExpensiveClass, c=123, d=456)\n",
    "    obj.print_result()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.12.1'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make PKL Global Average Pooling Data  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inceptionv3_pool3(src_folder , save_folder):\n",
    "    \"\"\"\n",
    "    src extension : npy \n",
    "    \"\"\"\n",
    "    np_path=glob.glob(src_folder+'*.npy')\n",
    "    pkl_name=path.replace('npy' , 'pkl').split('/')[-1]\n",
    "    pkl_save_path=os.path.join(pkl_savefolder ,pkl_name)\n",
    "    print pkl_save_path\n",
    "    np_img=np.load(np_path)\n",
    "    transfer_values_cache(pkl_save_path, images= np_img , model = model )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Make PKL CONV 8X8 , 2048 Data  "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pkl_savefolder = '/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl/'\n",
    "for path in crawl_folder('/ssd/n_vs_ab/type1/1/'):\n",
    "    if 'npy' in path  and 'img' in path:\n",
    "        pkl_name=path.replace('npy' , 'pkl').split('/')[-1]\n",
    "        pkl_save_path=os.path.join(pkl_savefolder ,pkl_name)\n",
    "        print pkl_save_path\n",
    "        img=np.load(path)\n",
    "        transfer_convJoin_values_cache(pkl_save_path, images= img , model = model )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Reduce Dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reduce_dimension(conv , out_ch  , layer_name):\n",
    "    in_ch=conv.get_shape()[-1]\n",
    "    with tf.variable_scope(layer_name) as scope:\n",
    "        w_conv=tf.get_variable(name = 'w_conv' , shape = [1,1,in_ch , out_ch] , initializer = tf.contrib.layers.xavier_initializer() ,dtype=tf.float32)\n",
    "        b_conv=tf.Variable([out_ch] , dtype=tf.float32)\n",
    "    layer=tf.nn.conv2d(conv , w_conv ,[1,2,2,1] , 'SAME') \n",
    "    layer=tf.add(layer  , b_conv)\n",
    "    layer = tf.nn.relu(layer)\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reduce_dimension_3x3(conv , out_ch  , layer_name):\n",
    "    in_ch=conv.get_shape()[-1]\n",
    "    with tf.variable_scope(layer_name) as scope:\n",
    "        w_conv=tf.get_variable(name = 'w_conv' , shape = [3,3,in_ch , out_ch] , initializer = tf.contrib.layers.xavier_initializer() ,dtype=tf.float32)\n",
    "        b_conv=tf.Variable([out_ch] , dtype=tf.float32)\n",
    "    layer=tf.nn.conv2d(conv , w_conv ,[1,2,2,1] , 'SAME') \n",
    "    layer=tf.add(layer  , b_conv)\n",
    "    layer = tf.nn.relu(layer)\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def reduce_dimension_5x5(conv , out_ch  , layer_name):\n",
    "    in_ch=conv.get_shape()[-1]\n",
    "    with tf.variable_scope(layer_name) as scope:\n",
    "        w_conv=tf.get_variable(name = 'w_conv' , shape = [5,5,in_ch , out_ch] , initializer = tf.contrib.layers.xavier_initializer() ,dtype=tf.float32)\n",
    "        b_conv=tf.Variable([out_ch] , dtype=tf.float32)\n",
    "    layer=tf.nn.conv2d(conv , w_conv ,[1,2,2,1] , 'SAME') \n",
    "    layer=tf.add(layer  , b_conv)\n",
    "    layer = tf.nn.relu(layer)\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Same Dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def same_dimension(conv , out_ch  , layer_name):\n",
    "    in_ch=conv.get_shape()[-1]\n",
    "    with tf.variable_scope(layer_name) as scope:\n",
    "        w_conv=tf.get_variable(name = 'w_conv' , shape = [1,1,in_ch , out_ch] , initializer = tf.contrib.layers.xavier_initializer() ,dtype=tf.float32)\n",
    "        b_conv=tf.Variable([out_ch] , dtype=tf.float32)\n",
    "    layer=tf.nn.conv2d(conv , w_conv ,[1,1,1,1] , 'SAME') \n",
    "    layer=tf.add(layer  , b_conv)\n",
    "    layer = tf.nn.relu(layer)\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def same_dimension_3x3(conv , out_ch  , layer_name):\n",
    "    in_ch=conv.get_shape()[-1]\n",
    "    with tf.variable_scope(layer_name) as scope:\n",
    "        w_conv=tf.get_variable(name = 'w_conv' , shape = [3,3,in_ch , out_ch] , initializer = tf.contrib.layers.xavier_initializer() ,dtype=tf.float32)\n",
    "        b_conv=tf.Variable([out_ch] , dtype=tf.float32)\n",
    "    layer=tf.nn.conv2d(conv , w_conv ,[1,1,1,1] , 'SAME') \n",
    "    layer=tf.add(layer  , b_conv)\n",
    "    layer = tf.nn.relu(layer)\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def same_dimension_5x5(conv , out_ch  , layer_name):\n",
    "    in_ch=conv.get_shape()[-1]\n",
    "    with tf.variable_scope(layer_name) as scope:\n",
    "        w_conv=tf.get_variable(name = 'w_conv' , shape = [5,5,in_ch , out_ch] , initializer = tf.contrib.layers.xavier_initializer() ,dtype=tf.float32)\n",
    "        b_conv=tf.Variable([out_ch] , dtype=tf.float32)\n",
    "    layer=tf.nn.conv2d(conv , w_conv ,[1,1,1,1] , 'SAME') \n",
    "    layer=tf.add(layer  , b_conv)\n",
    "    layer = tf.nn.relu(layer)\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_GAP(conv ,name, n_classes =2 ):\n",
    "    n_ch=conv.get_shape()[-1]\n",
    "    print conv.get_shape()\n",
    "    gap = tf.reduce_mean(conv , axis = (1,2))\n",
    "    init = tf.constant(np.random.rand(1,n_ch))\n",
    "    with tf.variable_scope(name) as scope:\n",
    "        gap_w=tf.get_variable('w' , shape = [ n_ch , n_classes])\n",
    "        gap_b=tf.get_variable('b' ,  initializer=init)\n",
    "        \n",
    "    logits=tf.matmul(gap , gap_w)\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fully_connect(flat_ ,out_ch , name):\n",
    "    _ , in_ch=flat_.get_shape()\n",
    "    with tf.variable_scope(name) as scope:\n",
    "        fc_w=tf.get_variable('w' , shape=[in_ch,out_ch])\n",
    "        fc_b=tf.get_variable('b'  , shape=[1,out_ch])\n",
    "    layer=tf.matmul(flat_ , fc_w)\n",
    "    layer=tf.add(layer , fc_b)\n",
    "    return layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_class_map(label , conv , img_width):\n",
    "    out_ch = int(conv.get_shape()[-1])\n",
    "    conv_resized_bilinear= tf.image.resize_bilinear(conv , [ im_width , im_width]) # interpolation\n",
    "\n",
    "    with tf.variable_scope('LeNet/GAP' , reuse = True):\n",
    "        tranpose_w=tf.transpose(tf.get_variable('w')) # (2,   2048)\n",
    "        _label_w = tf.gather(tf.transpose(tf.get_variable('w')) , label) # load tensor by name 'w'\n",
    "        label_w = tf.reshape(_label_w , [out_ch ,1 ]) #(2048 ,1)\n",
    "    conv_resized = tf.reshape(conv_resized_bilinear , [-1 , img_width*img_width , out_ch]) # 10000  , 2048\n",
    "    classmap_batch_mul=tf.map_fn( lambda x: tf.matmul(x , label_w), conv_resized) # batchsize , 100,100 ,1\n",
    "    #classmap_batch_mul = tf.batch_matmul(conv_resized, label_w)\n",
    "    classmap = tf.reshape(classmap_batch_mul , [-1, img_width , img_width])\n",
    "    return classmap , conv_resized, label_w , _label_w,tranpose_w,classmap_batch_mul,conv_resized_bilinear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def random_batch(x,y,batch_size):\n",
    "    num_images = len(x)\n",
    "    idx = np.random.choice(num_images , size=batch_size , replace = False)\n",
    "    y_batch = y[idx]\n",
    "    x_batch = x[idx]\n",
    "    return x_batch , y_batch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pickle2numpy(path):\n",
    "    f=open(path)\n",
    "    imgs=pickle.load(f)\n",
    "    return imgs  , path\n",
    "def load_all_train_RAM(train_pkl_paths):\n",
    "    row=8\n",
    "    col=8\n",
    "    n_ch=2048\n",
    "    n_classes=2\n",
    "    pool=Pool()\n",
    "    label_folder_path = '/ssd/n_vs_ab/type1/1/'\n",
    "    label_paths=crawl_folder(label_folder_path)\n",
    "    n=len(train_pkl_paths)\n",
    "    imgs_batch_list=[]\n",
    "    labs_batch_list=[]\n",
    "    count =0\n",
    "    for imgs ,path in pool.imap( pickle2numpy, train_pkl_paths):\n",
    "        name=path.split('/')[-1]\n",
    "        labs_name = name.replace('img.pkl' , 'lab.npy')\n",
    "        labs_path =os.path.join(label_folder_path , labs_name)\n",
    "        labs=np.load(labs_path)\n",
    "        imgs_batch_list.append(imgs)\n",
    "        labs_batch_list.append(labs)\n",
    "        msg = '\\r-Progress : {0} / {1}'.format(str(count) , str(n))\n",
    "        sys.stdout.write(msg)\n",
    "        sys.stdout.flush()\n",
    "        count+=1\n",
    "\n",
    "    return imgs_batch_list , labs_batch_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def choice_batch(imgs_batch_list , labs_batch_list):\n",
    "    n_batch=len(imgs_batch_list)\n",
    "    idx=random.randrange(0,n_batch)\n",
    "   \n",
    "    return imgs_batch_list[idx] , labs_batch_list[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_train(train_pkl_paths):\n",
    "    label_folder_path = '/ssd/n_vs_ab/type1/1/'\n",
    "    #print train_pkl_paths\n",
    "    idx=random.randrange(0,len(train_pkl_paths)-15)    \n",
    "    train_pkl_path=train_pkl_paths[idx]\n",
    "    name=train_pkl_path.split('/')[-1]\n",
    "    lab_name = name.replace('img.pkl' , 'lab.npy')\n",
    "    train_label_path =os.path.join(label_folder_path , lab_name)\n",
    "    train_img=open(train_pkl_path , mode = 'rb') \n",
    "    train_img= pickle.load(train_img)\n",
    "    train_lab= np.load(train_label_path)\n",
    "    #print train_label_path\n",
    "    #print train_pkl_path\n",
    "    train_img=np.squeeze(train_img )\n",
    "    return train_img , train_lab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_val(imgs_batch_list , labs_batch_list):\n",
    "    batch_img_lab = zip(imgs_batch_list , labs_batch_list)\n",
    "    train_imgs=[]\n",
    "    train_labs=[]\n",
    "    for img,lab in (batch_img_lab): \n",
    "        img=np.squeeze(img)\n",
    "        print np.shape(img)\n",
    "        train_imgs.append(list(img))\n",
    "        train_labs.append(list(lab))\n",
    "    shape=np.shape(train_imgs)\n",
    "    print shape[0]\n",
    "    print shape[1]\n",
    "    train_imgs=np.reshape(train_imgs , newshape = (shape[0]*shape[1] ,shape[2],shape[3],-1))\n",
    "    train_labs=np.reshape(train_labs , newshape = (shape[0]*shape[1] ,2))\n",
    "    print np.shape(train_labs)\n",
    "    print np.shape(train_imgs)\n",
    "    #train_imgs =np.reshape(train_imgs,newshape=(15*80 , 2048))\n",
    "    #train_labs =np.reshape(train_labs,newshape=(15*80 , n_classes))\n",
    "    return train_imgs , train_labs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "global_step = tf.Variable(initial_value=0 , name='global_step' , trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_algo(y_conv,y_ , learning_rate):\n",
    "    y_pred=tf.nn.softmax(y_conv)\n",
    "    y_pred_cls = tf.argmax(y_pred , dimension=1)\n",
    "    y_cls = tf.argmax(y_ , dimension =1)\n",
    "    correct_prediction= tf.equal(y_pred_cls , y_cls)\n",
    "    accuracy=tf.reduce_mean(tf.cast(correct_prediction , tf.float32))\n",
    "    cost  = tf.nn.softmax_cross_entropy_with_logits(y_conv , y_)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)\n",
    "    return accuracy , y_pred , cost ,optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Add_3:0\", shape=(?, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "A=same_dimension(x_ , 1024 ,'A')\n",
    "B=same_dimension(A , 512, 'B')\n",
    "C=same_dimension(B , 256,'C')\n",
    "flat_C = tf.contrib.layers.flatten(C)\n",
    "y_conv=fully_connect(flat_C , 2 ,'fc1')\n",
    "print y_conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "accuracy,y_pred,cost,optimizer = train_algo(y_conv ,y_ , 0.001)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "y_pred=tf.nn.softmax(y_conv)\n",
    "y_pred_cls = tf.argmax(y_pred , dimension=1)\n",
    "y_cls = tf.argmax(y_ , dimension =1)\n",
    "correct_prediction= tf.equal(y_pred_cls , y_cls)\n",
    "accuracy=tf.reduce_mean(tf.cast(correct_prediction , tf.float32))\n",
    "cost  = tf.nn.softmax_cross_entropy_with_logits(y_conv , y_)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = 0.0001).minimize(cost,global_step)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "A_1=same_dimension(x_ , 1024 ,'A_1')\n",
    "B_1=reduce_dimension(A_1 , 512, 'B_1')\n",
    "C_1=reduce_dimension(B_1 , 256,'C_1')\n",
    "flat_C_1 = tf.contrib.layers.flatten(C_1)\n",
    "y_conv_1=fully_connect(flat_C_1 , 2 ,'fc1_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "accuracy_1 , y_pred_1 , cost_1,optimizer_1 = train_algo(y_conv_1 ,y_ , 0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global_Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "global_step = tf.Variable(initial_value=0 , name='global_step' , trainable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size =32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load PKL PRE-Train image Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_0_img.pkl\n",
      "0\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_100_img.pkl\n",
      "1\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_101_img.pkl\n",
      "2\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_102_img.pkl\n",
      "3\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_103_img.pkl\n",
      "4\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_104_img.pkl\n",
      "5\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_105_img.pkl\n",
      "6\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_106_img.pkl\n",
      "7\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_107_img.pkl\n",
      "8\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_108_img.pkl\n",
      "9\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_109_img.pkl\n",
      "10\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_10_img.pkl\n",
      "11\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_110_img.pkl\n",
      "12\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_111_img.pkl\n",
      "13\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_112_img.pkl\n",
      "14\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_113_img.pkl\n",
      "15\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_209_img.pkl\n",
      "16\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_20_img.pkl\n",
      "17\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_210_img.pkl\n",
      "18\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_211_img.pkl\n",
      "19\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_212_img.pkl\n",
      "20\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_213_img.pkl\n",
      "21\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_214_img.pkl\n",
      "22\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_215_img.pkl\n",
      "23\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_216_img.pkl\n",
      "24\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_217_img.pkl\n",
      "25\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_218_img.pkl\n",
      "26\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_219_img.pkl\n",
      "27\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_21_img.pkl\n",
      "28\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_220_img.pkl\n",
      "29\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_221_img.pkl\n",
      "30\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_222_img.pkl\n",
      "31\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_40_img.pkl\n",
      "32\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_41_img.pkl\n",
      "33\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_42_img.pkl\n",
      "34\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_43_img.pkl\n",
      "35\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_44_img.pkl\n",
      "36\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_45_img.pkl\n",
      "37\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_46_img.pkl\n",
      "38\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_47_img.pkl\n",
      "39\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_48_img.pkl\n",
      "40\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_49_img.pkl\n",
      "41\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_4_img.pkl\n",
      "42\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_50_img.pkl\n",
      "43\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_51_img.pkl\n",
      "44\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_52_img.pkl\n",
      "45\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_53_img.pkl\n",
      "46\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_54_img.pkl\n",
      "47\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_56_img.pkl\n",
      "48\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_57_img.pkl\n",
      "49\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_58_img.pkl\n",
      "50\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_59_img.pkl\n",
      "51\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_5_img.pkl\n",
      "52\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_60_img.pkl\n",
      "53\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_61_img.pkl\n",
      "54\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_62_img.pkl\n",
      "55\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_63_img.pkl\n",
      "56\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_64_img.pkl\n",
      "57\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_65_img.pkl\n",
      "58\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_66_img.pkl\n",
      "59\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_67_img.pkl\n",
      "60\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_68_img.pkl\n",
      "61\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_69_img.pkl\n",
      "62\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_6_img.pkl\n",
      "63\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_114_img.pkl\n",
      "64\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_133_img.pkl\n",
      "65\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_149_img.pkl\n",
      "66\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_164_img.pkl\n",
      "67\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_17_img.pkl\n",
      "68\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_208_img.pkl\n",
      "69\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_223_img.pkl\n",
      "70\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_239_img.pkl\n",
      "71\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_255_img.pkl\n",
      "72\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_272_img.pkl\n",
      "73\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_288_img.pkl\n",
      "74\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_302_img.pkl\n",
      "75\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_318_img.pkl\n",
      "76\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_333_img.pkl\n",
      "77\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_3_img.pkl\n",
      "78\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_55_img.pkl\n",
      "79\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_70_img.pkl\n",
      "80\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_71_img.pkl\n",
      "81\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_72_img.pkl\n",
      "82\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_73_img.pkl\n",
      "83\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_74_img.pkl\n",
      "84\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_75_img.pkl\n",
      "85\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_76_img.pkl\n",
      "86\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_77_img.pkl\n",
      "87\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_78_img.pkl\n",
      "88\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_79_img.pkl\n",
      "89\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_7_img.pkl\n",
      "90\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_80_img.pkl\n",
      "91\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_81_img.pkl\n",
      "92\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_82_img.pkl\n",
      "93\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_83_img.pkl\n",
      "94\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_84_img.pkl\n",
      "95\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_85_img.pkl\n",
      "96\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_86_img.pkl\n",
      "97\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_87_img.pkl\n",
      "98\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_88_img.pkl\n",
      "99\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_89_img.pkl\n",
      "100\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_8_img.pkl\n",
      "101\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_90_img.pkl\n",
      "102\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_91_img.pkl\n",
      "103\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_92_img.pkl\n",
      "104\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_93_img.pkl\n",
      "105\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_94_img.pkl\n",
      "106\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_95_img.pkl\n",
      "107\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_96_img.pkl\n",
      "108\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_97_img.pkl\n",
      "109\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_98_img.pkl\n",
      "110\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_99_img.pkl\n",
      "111\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_9_img.pkl\n",
      "112\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_134_img.pkl\n",
      "113\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_135_img.pkl\n",
      "114\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_136_img.pkl\n",
      "115\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_137_img.pkl\n",
      "116\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_138_img.pkl\n",
      "117\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_139_img.pkl\n",
      "118\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_13_img.pkl\n",
      "119\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_140_img.pkl\n",
      "120\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_141_img.pkl\n",
      "121\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_142_img.pkl\n",
      "122\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_143_img.pkl\n",
      "123\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_144_img.pkl\n",
      "124\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_145_img.pkl\n",
      "125\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_146_img.pkl\n",
      "126\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_147_img.pkl\n",
      "127\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_148_img.pkl\n",
      "128\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_115_img.pkl\n",
      "129\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_116_img.pkl\n",
      "130\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_117_img.pkl\n",
      "131\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_118_img.pkl\n",
      "132\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_119_img.pkl\n",
      "133\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_11_img.pkl\n",
      "134\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_120_img.pkl\n",
      "135\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_121_img.pkl\n",
      "136\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_122_img.pkl\n",
      "137\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_123_img.pkl\n",
      "138\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_124_img.pkl\n",
      "139\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_125_img.pkl\n",
      "140\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_126_img.pkl\n",
      "141\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_127_img.pkl\n",
      "142\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_128_img.pkl\n",
      "143\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_129_img.pkl\n",
      "144\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_12_img.pkl\n",
      "145\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_130_img.pkl\n",
      "146\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_131_img.pkl\n",
      "147\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_132_img.pkl\n",
      "148\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_14_img.pkl\n",
      "149\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_150_img.pkl\n",
      "150\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_151_img.pkl\n",
      "151\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_152_img.pkl\n",
      "152\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_153_img.pkl\n",
      "153\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_154_img.pkl\n",
      "154\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_155_img.pkl\n",
      "155\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_156_img.pkl\n",
      "156\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_157_img.pkl\n",
      "157\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_158_img.pkl\n",
      "158\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_159_img.pkl\n",
      "159\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_15_img.pkl\n",
      "160\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_160_img.pkl\n",
      "161\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_161_img.pkl\n",
      "162\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_162_img.pkl\n",
      "163\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_163_img.pkl\n",
      "164\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_165_img.pkl\n",
      "165\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_166_img.pkl\n",
      "166\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_167_img.pkl\n",
      "167\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_168_img.pkl\n",
      "168\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_169_img.pkl\n",
      "169\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_16_img.pkl\n",
      "170\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_170_img.pkl\n",
      "171\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_171_img.pkl\n",
      "172\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_172_img.pkl\n",
      "173\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_173_img.pkl\n",
      "174\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_174_img.pkl\n",
      "175\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_175_img.pkl\n",
      "176\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_176_img.pkl\n",
      "177\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_177_img.pkl\n",
      "178\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_178_img.pkl\n",
      "179\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_179_img.pkl\n",
      "180\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_180_img.pkl\n",
      "181\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_181_img.pkl\n",
      "182\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_182_img.pkl\n",
      "183\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_183_img.pkl\n",
      "184\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_184_img.pkl\n",
      "185\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_185_img.pkl\n",
      "186\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_186_img.pkl\n",
      "187\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_187_img.pkl\n",
      "188\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_188_img.pkl\n",
      "189\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_189_img.pkl\n",
      "190\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_18_img.pkl\n",
      "191\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_190_img.pkl\n",
      "192\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_191_img.pkl\n",
      "193\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_192_img.pkl\n",
      "194\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_193_img.pkl\n",
      "195\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_194_img.pkl\n",
      "196\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_195_img.pkl\n",
      "197\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_196_img.pkl\n",
      "198\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_197_img.pkl\n",
      "199\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_198_img.pkl\n",
      "200\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_199_img.pkl\n",
      "201\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_19_img.pkl\n",
      "202\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_1_img.pkl\n",
      "203\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_200_img.pkl\n",
      "204\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_201_img.pkl\n",
      "205\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_202_img.pkl\n",
      "206\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_203_img.pkl\n",
      "207\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_204_img.pkl\n",
      "208\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_205_img.pkl\n",
      "209\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_206_img.pkl\n",
      "210\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_207_img.pkl\n",
      "211\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_224_img.pkl\n",
      "212\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_225_img.pkl\n",
      "213\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_226_img.pkl\n",
      "214\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_227_img.pkl\n",
      "215\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_228_img.pkl\n",
      "216\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_229_img.pkl\n",
      "217\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_22_img.pkl\n",
      "218\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_230_img.pkl\n",
      "219\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_231_img.pkl\n",
      "220\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_232_img.pkl\n",
      "221\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_233_img.pkl\n",
      "222\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_234_img.pkl\n",
      "223\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_235_img.pkl\n",
      "224\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_236_img.pkl\n",
      "225\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_237_img.pkl\n",
      "226\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_238_img.pkl\n",
      "227\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_23_img.pkl\n",
      "228\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_240_img.pkl\n",
      "229\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_241_img.pkl\n",
      "230\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_242_img.pkl\n",
      "231\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_243_img.pkl\n",
      "232\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_244_img.pkl\n",
      "233\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_245_img.pkl\n",
      "234\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_246_img.pkl\n",
      "235\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_247_img.pkl\n",
      "236\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_248_img.pkl\n",
      "237\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_249_img.pkl\n",
      "238\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_24_img.pkl\n",
      "239\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_250_img.pkl\n",
      "240\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_251_img.pkl\n",
      "241\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_252_img.pkl\n",
      "242\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_253_img.pkl\n",
      "243\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_254_img.pkl\n",
      "244\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_256_img.pkl\n",
      "245\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_257_img.pkl\n",
      "246\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_258_img.pkl\n",
      "247\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_259_img.pkl\n",
      "248\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_25_img.pkl\n",
      "249\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_260_img.pkl\n",
      "250\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_261_img.pkl\n",
      "251\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_262_img.pkl\n",
      "252\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_263_img.pkl\n",
      "253\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_264_img.pkl\n",
      "254\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_265_img.pkl\n",
      "255\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_266_img.pkl\n",
      "256\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_267_img.pkl\n",
      "257\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_268_img.pkl\n",
      "258\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_269_img.pkl\n",
      "259\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_26_img.pkl\n",
      "260\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_270_img.pkl\n",
      "261\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_271_img.pkl\n",
      "262\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_273_img.pkl\n",
      "263\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_274_img.pkl\n",
      "264\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_275_img.pkl\n",
      "265\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_276_img.pkl\n",
      "266\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_277_img.pkl\n",
      "267\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_278_img.pkl\n",
      "268\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_279_img.pkl\n",
      "269\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_27_img.pkl\n",
      "270\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_280_img.pkl\n",
      "271\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_281_img.pkl\n",
      "272\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_282_img.pkl\n",
      "273\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_283_img.pkl\n",
      "274\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_284_img.pkl\n",
      "275\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_285_img.pkl\n",
      "276\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_286_img.pkl\n",
      "277\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_287_img.pkl\n",
      "278\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_289_img.pkl\n",
      "279\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_28_img.pkl\n",
      "280\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_290_img.pkl\n",
      "281\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_291_img.pkl\n",
      "282\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_292_img.pkl\n",
      "283\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_293_img.pkl\n",
      "284\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_294_img.pkl\n",
      "285\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_295_img.pkl\n",
      "286\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_296_img.pkl\n",
      "287\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_297_img.pkl\n",
      "288\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_298_img.pkl\n",
      "289\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_299_img.pkl\n",
      "290\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_29_img.pkl\n",
      "291\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_2_img.pkl\n",
      "292\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_300_img.pkl\n",
      "293\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_301_img.pkl\n",
      "294\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_303_img.pkl\n",
      "295\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_304_img.pkl\n",
      "296\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_305_img.pkl\n",
      "297\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_306_img.pkl\n",
      "298\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_307_img.pkl\n",
      "299\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_308_img.pkl\n",
      "300\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_309_img.pkl\n",
      "301\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_30_img.pkl\n",
      "302\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_310_img.pkl\n",
      "303\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_311_img.pkl\n",
      "304\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_312_img.pkl\n",
      "305\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_313_img.pkl\n",
      "306\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_314_img.pkl\n",
      "307\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_315_img.pkl\n",
      "308\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_316_img.pkl\n",
      "309\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_317_img.pkl\n",
      "310\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_319_img.pkl\n",
      "311\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_31_img.pkl\n",
      "312\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_320_img.pkl\n",
      "313\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_321_img.pkl\n",
      "314\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_322_img.pkl\n",
      "315\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_323_img.pkl\n",
      "316\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_324_img.pkl\n",
      "317\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_325_img.pkl\n",
      "318\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_326_img.pkl\n",
      "319\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_327_img.pkl\n",
      "320\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_328_img.pkl\n",
      "321\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_329_img.pkl\n",
      "322\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_32_img.pkl\n",
      "323\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_330_img.pkl\n",
      "324\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_331_img.pkl\n",
      "325\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_332_img.pkl\n",
      "326\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_334_img.pkl\n",
      "327\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_335_img.pkl\n",
      "328\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_336_img.pkl\n",
      "329\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_337_img.pkl\n",
      "330\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_338_img.pkl\n",
      "331\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_339_img.pkl\n",
      "332\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_33_img.pkl\n",
      "333\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_340_img.pkl\n",
      "334\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_341_img.pkl\n",
      "335\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_342_img.pkl\n",
      "336\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_343_img.pkl\n",
      "337\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_344_img.pkl\n",
      "338\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_345_img.pkl\n",
      "339\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_346_img.pkl\n",
      "340\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_34_img.pkl\n",
      "341\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_35_img.pkl\n",
      "342\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_36_img.pkl\n",
      "343\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_37_img.pkl\n",
      "344\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_38_img.pkl\n",
      "345\n",
      "/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl//train_39_img.pkl\n",
      "346\n"
     ]
    }
   ],
   "source": [
    "all_paths=crawl_folder('/ssd/n_vs_ab/type1/1_conv_8x8_2048_pkl/')\n",
    "train_pkl_paths =[]\n",
    "count =0\n",
    "for i,path in enumerate(all_paths):\n",
    "    if 'train' in path and '.pkl' in path:\n",
    "        train_pkl_paths.append(path)\n",
    "        print path\n",
    "        print count\n",
    "        count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#train_pkl_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # total batch Pkl Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "347"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_pkl_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load All Train Data to RAM Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-Progress : 346 / 347"
     ]
    }
   ],
   "source": [
    "imgs_batch_list,labs_batch_list=load_all_train_RAM(train_pkl_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Validation Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val_img,val_lab=load_val(imgs_batch_list[:-15],labs_batch_list[:-15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-37-25ffbcbda69c>:2 in <module>.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "init = tf.initialize_all_variables()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def optimize(iteration):\n",
    "    start_time = time.time()\n",
    "    for i in range(iteration):\n",
    "        #val_feed={x_: val_img , y_ : val_lab }\n",
    "        train_img , train_lab=choice_batch(imgs_batch_list[:-15] , labs_batch_list[:-15])\n",
    "        train_img=np.squeeze(train_img)\n",
    "        x_batch , y_batch = random_batch(train_img,train_lab ,batch_size)\n",
    "        feed_dict_train= {x_:x_batch , y_:y_batch}\n",
    "        _ = sess.run([optimizer] ,  feed_dict=feed_dict_train)\n",
    "        _ = sess.run([optimizer_1] ,  feed_dict=feed_dict_train)\n",
    "        if (i %200 ==0 ) or (i ==iteration -1):\n",
    "            \n",
    "            batch_acc  , pred , loss = sess.run([accuracy , y_pred , cost ] , feed_dict = feed_dict_train)\n",
    "            batch_acc_1  , pred_1 , loss_1 = sess.run([accuracy_1 , y_pred_1 , cost_1 ] , feed_dict = feed_dict_train)\n",
    "\n",
    "            share= len(val_img) /batch_size\n",
    "            remainder= len(val_img) %batch_size\n",
    "            val_loss_mean=[]\n",
    "            val_loss_mean_1=[]\n",
    "            for i in range(share):\n",
    "                feed_dict_val={x_: val_img[i:(i+1)*batch_size], y_:val_lab[i:(i+1)*batch_size]}\n",
    "                val_acc  , va_pred , val_loss = sess.run([accuracy , y_pred , cost ] , feed_dict = feed_dict_val)\n",
    "                val_acc_1  , va_pred_1 ,val_loss_1 = sess.run([accuracy_1 , y_pred_1 , cost_1 ] , feed_dict = feed_dict_val)\n",
    "                val_loss_mean.append(np.asarray(val_loss))\n",
    "                val_loss_mean_1.append(np.asarray(val_loss_1))\n",
    "            feed_dict_val={x_: val_img[i:(i+1)*batch_size], y_:val_lab[i:(i+1)*batch_size]}\n",
    "            val_acc  , va_pred , val_loss = sess.run([accuracy , y_pred , cost ] , feed_dict = feed_dict_val)\n",
    "            val_acc_1  , va_pred_1 ,val_loss_1 = sess.run([accuracy_1 , y_pred_1 , cost_1 ] , feed_dict = feed_dict_val)\n",
    "            val_loss_mean=train_loss_mean.mean()\n",
    "            val_loss_mean_1=train_loss_mean_1,mean()\n",
    "            msg = \"Model 1 :Global Step : {0:>6} , Training Batch Accuracy: {1:>6.1%} , Training  Loss: {2}\\n Validation Accuracy: {3:>6.1%} , Validation Loss:{4}\"\n",
    "            print (msg.format(i , batch_acc ,train_loss_mean , val_acc , val_loss_mean))\n",
    "            msg = \"Model 2 :Global Step : {0:>6} , Training Batch Accuracy: {1:>6.1%} , Training  Loss: {2}\\n Validation Accuracy: {3:>6.1%} , Validation Loss:{4}\"\n",
    "            print (msg.format(i , batch_acc_1 ,train_loss_mean_1 , val_acc_1 , val_loss_mean_1))\n",
    "\n",
    "    end_time = time.time()\n",
    "    time_dif = end_time - start_time\n",
    "    print (\"Time usage:\"+str(timedelta(seconds=int(round(time_dif)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "optimize(iteration=30000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_validated(train_pkl_paths):\n",
    "    label_folder_path = '/ssd/n_vs_ab/type1/1/'\n",
    "    pred_list=[]\n",
    "    #print train_pkl_paths\n",
    "    for idx in range(len(train_pkl_paths)-15 ,len(train_pkl_paths)) :\n",
    "        print idx\n",
    "        val_pkl_path=train_pkl_paths[idx]\n",
    "        name=val_pkl_path.split('/')[-1]\n",
    "        lab_name = name.replace('img.pkl' , 'lab.npy')\n",
    "        val_pkl_path =os.path.join(label_folder_path , lab_name)\n",
    "        val_img=open(val_pkl_path , mode = 'rb') \n",
    "        val_img= pickle.load(val_img)\n",
    "        val_lab= np.load(val_label_path)\n",
    "        feed_dict_train= {x_:val_img , y_:val_lab}\n",
    "        pred = sess.run([y_pred] , feed_dict = feed_dict_train)\n",
    "        pred_list.extend(pred)\n",
    "    acc = pred_list.mean()\n",
    "        #print train_label_path\n",
    "        #print train_pkl_path\n",
    "    return pred_list , acc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "range(len(train_pkl_paths)-15 ,len(train_pkl_paths))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
